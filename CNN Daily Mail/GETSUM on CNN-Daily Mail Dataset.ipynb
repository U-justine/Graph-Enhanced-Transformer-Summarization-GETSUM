{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPn4hI2AdnwFF4/LTbsCLJa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4H1_jSgzETwv"},"outputs":[],"source":["!pip install -q transformers datasets rouge-score bert-score spacy tqdm\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","source":["import os\n","import math\n","import random\n","from tqdm.auto import tqdm\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import BertTokenizerFast, BertModel\n","from datasets import load_dataset\n","from rouge_score import rouge_scorer\n","from bert_score import score as bertscore_score\n","\n","import spacy\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"2LFBLWFsG0U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)"],"metadata":{"id":"rhMwr0UjG3Fg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- Hyperparams ----------\n","NUM_SAMPLES = 1000\n","SENT_MAX_TOKENS = 128\n","SIM_THRESHOLD = 0.30\n","GAT_HID = 256\n","BERT_MODEL = \"bert-base-uncased\"\n","TOP_K_SENT = 3\n","BATCH_ENCODING = 32"],"metadata":{"id":"qHysZmfLG7H4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=f\"test[:{NUM_SAMPLES}]\")\n","print(\"Loaded dataset samples:\", len(dataset))"],"metadata":{"id":"X0_7gVl5HGXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utility: sentence-splitter\n","def split_sentences(article):\n","    doc = nlp(article)\n","    sents = [s.text.strip() for s in doc.sents if len(s.text.strip())>10]  # remove very short noisy sentences\n","    return sents if len(sents)>0 else [article[:300]]  # fallback"],"metadata":{"id":"9XyNF7e_HMeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT sentence encoder (averaged token embeddings)\n","tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)\n","bert = BertModel.from_pretrained(BERT_MODEL).to(device)\n","bert.eval()"],"metadata":{"id":"gYhYmHrtHMS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def encode_sentences(sent_list):\n","    \"\"\"\n","    sentence_list: list[str] -> returns numpy array (N, hidden)\n","    encodes sentences in batches using bert; returns CLS-pooled embedding (or mean token embeddings).\n","    \"\"\"\n","    embs = []\n","    for i in range(0, len(sent_list), BATCH_ENCODING):\n","        batch = sent_list[i:i+BATCH_ENCODING]\n","        encoded = tokenizer(batch, truncation=True, padding=True, max_length=SENT_MAX_TOKENS, return_tensors=\"pt\")\n","        input_ids = encoded[\"input_ids\"].to(device)\n","        attn = encoded[\"attention_mask\"].to(device)\n","        out = bert(input_ids=input_ids, attention_mask=attn)\n","        # use mean pooling of last_hidden_state (excluding padding)\n","        last = out.last_hidden_state  # (B, L, H)\n","        mask = attn.unsqueeze(-1)     # (B, L, 1)\n","        summed = (last * mask).sum(1) # (B, H)\n","        denom = mask.sum(1).clamp(min=1e-9)\n","        mean_pooled = (summed / denom).cpu().numpy()\n","        embs.append(mean_pooled)\n","    return np.vstack(embs)  # (N, H)\n"],"metadata":{"id":"Lv-rtoE-HcOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GAT implementation (2-layer)\n","class GATLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim):\n","        super().__init__()\n","        self.W = nn.Linear(in_dim, out_dim, bias=False)\n","        self.a = nn.Linear(2*out_dim, 1, bias=False)\n","        self.leaky = nn.LeakyReLU(0.2)\n","    def forward(self, h, adj_list):\n","        # h: (N, in_dim). adj_list: list of neighbor idx lists for each node\n","        Wh = self.W(h)  # (N, out_dim)\n","        N = Wh.size(0)\n","        Wh_repeat_i = []\n","        Wh_repeat_j = []\n","        e_rows = []\n","        # compute attention e_ij only for edges in adj_list to save compute\n","        for i in range(N):\n","            neigh = adj_list[i]\n","            if len(neigh)==0:\n","                e_rows.append((i, torch.tensor([], device=Wh.device, dtype=torch.float)))\n","                continue\n","            wi = Wh[i].unsqueeze(0).repeat(len(neigh),1)  # (deg, out)\n","            wj = Wh[neigh]  # (deg, out)\n","            a_input = torch.cat([wi, wj], dim=1)  # (deg, 2*out)\n","            e_ij = self.leaky(self.a(a_input)).squeeze(-1)  # (deg,)\n","            e_rows.append((i, e_ij))\n","        # softmax over neighbors and compute aggregated features\n","        out = torch.zeros_like(Wh)\n","        for i, e_ij in e_rows:\n","            neigh = adj_list[i]\n","            if len(neigh)==0:\n","                out[i] = Wh[i]  # self-loop fallback\n","                continue\n","            alpha = F.softmax(e_ij, dim=0)  # (deg,)\n","            neigh_feat = Wh[neigh]  # (deg, out)\n","            agg = (alpha.unsqueeze(1) * neigh_feat).sum(0)  # (out,)\n","            out[i] = agg\n","        return out"],"metadata":{"id":"MCRrG25PHrB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GETSumGAT(nn.Module):\n","    def __init__(self, in_dim, hid_dim):\n","        super().__init__()\n","        self.gat1 = GATLayer(in_dim, hid_dim)\n","        self.gat2 = GATLayer(hid_dim, hid_dim)\n","    def forward(self, h, adj_list):\n","        h1 = F.elu(self.gat1(h, adj_list))\n","        h2 = self.gat2(h1, adj_list)\n","        return h2  # (N, hid_dim)"],"metadata":{"id":"pmNl5mh1HybJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ranking head and gating\n","class GETSumModel(nn.Module):\n","    def __init__(self, sent_dim, gat_hid):\n","        super().__init__()\n","        self.gat = GETSumGAT(sent_dim, gat_hid)\n","        # gating: combine sent emb and gat emb\n","        self.gate = nn.Sequential(\n","            nn.Linear(sent_dim + gat_hid, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1)\n","        )\n","        self.scorer = nn.Linear(sent_dim + gat_hid, 1)\n","    def forward(self, sent_emb_np, adj_list):\n","        # sent_emb_np: numpy (N, sent_dim); convert to tensor\n","        h = torch.from_numpy(sent_emb_np).float().to(device)\n","        gat_h = self.gat(h, adj_list)  # (N, gat_hid)\n","        concat = torch.cat([h, gat_h], dim=1)  # (N, sent+gat)\n","        # gating (sigmoid) to mix (we'll use gate scalar per sentence)\n","        gate_logits = self.gate(concat).squeeze(-1)  # (N,)\n","        gate = torch.sigmoid(gate_logits).unsqueeze(1)  # (N,1)\n","        # joint representation\n","        sent_dim = h.size(1)\n","        gat_dim = gat_h.size(1)\n","        # to combine we will compute weighted sum: gate * sent + (1-gate) * gat_projected\n","        # project gat to sent_dim if dims differ; here dims may be different, so just keep concat for scoring\n","        scores = self.scorer(concat).squeeze(-1)  # (N,)\n","        return scores.detach().cpu().numpy(), gate.detach().cpu().numpy(), concat.detach().cpu().numpy()"],"metadata":{"id":"xsnhLi2CH2ZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper: build adjacency list per document using cosine sim threshold\n","def build_adj_list(sent_embs, threshold=SIM_THRESHOLD):\n","    # sent_embs: (N, H)\n","    N = sent_embs.shape[0]\n","    if N==1:\n","        return [[]]  # no edges\n","    sim = cosine_similarity(sent_embs)  # (N,N)\n","    adj_list = []\n","    for i in range(N):\n","        # neighbors excluding self where sim >= threshold\n","        neigh = [j for j in range(N) if j!=i and sim[i,j] >= threshold]\n","        # if none, include top-2 most similar to keep graph connected\n","        if len(neigh)==0:\n","            topk = np.argsort(sim[i])[::-1][1:3]  # skip self\n","            neigh = [int(x) for x in topk]\n","        adj_list.append(neigh)\n","    return adj_list\n"],"metadata":{"id":"2kS8gcSUH97B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize GETSum model (weights on CPU then move to device)\n","SENT_DIM = bert.config.hidden_size  # typically 768\n","model_getsum = GETSumModel(sent_dim=SENT_DIM, gat_hid=GAT_HID).to(device)\n","model_getsum.eval()"],"metadata":{"id":"c5SVUn5nIQnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run inference over dataset, build summaries, and evaluate\n","references = []\n","predictions = []\n","sample_count = len(dataset)"],"metadata":{"id":"gnHdXwwTIJTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in tqdm(range(sample_count), desc=\"Processing articles\"):\n","    sample = dataset[idx]\n","    article = sample[\"article\"]\n","    ref = sample[\"highlights\"]\n","    sentences = split_sentences(article)\n","    # limit sentence count for very long docs to avoid explosion (optional)\n","    if len(sentences) > 80:\n","        # keep first 60 and top 20 longest (heuristic)\n","        sentences = sentences[:60] + sorted(sentences[60:], key=len, reverse=True)[:20]\n","    sent_embs = encode_sentences(sentences)  # (n_sent, H)\n","    adj_list = build_adj_list(sent_embs, threshold=SIM_THRESHOLD)\n","    # forward through GETSum scoring head\n","    with torch.no_grad():\n","        scores, gate_vals, joint_reps = model_getsum(sent_embs, adj_list)\n","    # pick top-K sentences by score (maintain original order)\n","    topk_idx = np.argsort(scores)[-TOP_K_SENT:]\n","    topk_idx_sorted = sorted(topk_idx)\n","    summary = \" \".join([sentences[i] for i in topk_idx_sorted])\n","    references.append(ref)\n","    predictions.append(summary)\n"],"metadata":{"id":"MZEEOvXSIbDT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation: ROUGE\n","scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n","r1 = r2 = rl = 0.0\n","for ref, pred in zip(references, predictions):\n","    sc = scorer.score(ref, pred)\n","    r1 += sc['rouge1'].fmeasure\n","    r2 += sc['rouge2'].fmeasure\n","    rl += sc['rougeL'].fmeasure\n","n = len(predictions)\n","print(f\"\\nROUGE-1: {r1/n:.4f}, ROUGE-2: {r2/n:.4f}, ROUGE-L: {rl/n:.4f}\")"],"metadata":{"id":"HMR3TzQGI3G1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluation: BERTScore\n","P, R, F1 = bertscore_score(predictions, references, lang=\"en\", verbose=True)\n","print(\"BERTScore F1 (mean):\", F1.mean().item())"],"metadata":{"id":"F1YKcTveI7TO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show a few examples\n","print(\"\\n--- Examples ---\\n\")\n","for i in range(3):\n","    print(\"ARTICLE (start):\", dataset[i][\"article\"][:400].replace(\"\\n\",\" \"), \"...\\n\")\n","    print(\"REFERENCE:\", references[i], \"\\n\")\n","    print(\"GETSum (extract):\", predictions[i], \"\\n\")\n","    print(\"-\"*80)"],"metadata":{"id":"qboUtL79I_ql"},"execution_count":null,"outputs":[]}]}