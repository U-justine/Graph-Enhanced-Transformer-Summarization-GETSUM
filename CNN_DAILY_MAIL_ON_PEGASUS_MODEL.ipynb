{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mYx3_FapPPL"
      },
      "outputs": [],
      "source": [
        "# Cell 1 - Imports\n",
        "import numpy as np\n",
        "import nltk\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    PegasusTokenizer,\n",
        "    PegasusForConditionalGeneration,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from evaluate import load\n",
        "\n",
        "# Download punkt for sentence splitting if needed\n",
        "nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Load CNN/DailyMail dataset (FULL, not subset)\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"default\")\n",
        "\n",
        "print(dataset)\n",
        "print(\"Train size:\", len(dataset[\"train\"]))\n",
        "print(\"Validation size:\", len(dataset[\"validation\"]))\n",
        "print(\"Test size:\", len(dataset[\"test\"]))\n"
      ],
      "metadata": {
        "id": "uKw3q3E0paPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - Load Pegasus tokenizer & model\n",
        "model_name = \"google/pegasus-large\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Mzprwyccpce9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 - Preprocessing\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"article\"]\n",
        "    targets = examples[\"highlights\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Labels\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"])\n"
      ],
      "metadata": {
        "id": "bJHXvex5pe0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 - Training setup\n",
        "batch_size = 4  # Increase if you have more GPUs\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./pegasus_cnn\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=1,   # change to more epochs if you have compute\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),   # use mixed precision if GPU supports\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "jsqtaMF5piRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - Metrics\n",
        "rouge = load(\"rouge\")\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # ROUGE\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # BERTScore\n",
        "    bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "\n",
        "    # Return average\n",
        "    result = {\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"])\n",
        "    }\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "gEkAhqDupmth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - Trainer\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],        # full train set\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],    # full val set\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pfhn__tJppVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - Training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "3zAb2Ynvprsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 - Evaluate on FULL Test set\n",
        "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "GBkUUxSepuoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}