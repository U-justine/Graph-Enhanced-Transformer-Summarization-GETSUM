{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5XiQRzefgzN"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install if needed\n",
        "# !pip install transformers datasets torch dgl spacy scikit-learn\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import dgl\n",
        "from dgl.nn.pytorch import GATConv\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire dataset (train, validation, test splits)\n",
        "dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Print dataset info\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "xz1SCUnlftKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function as per document: normalization, segmentation, tokenization, stopword filtering, rare token handling\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_article(article):\n",
        "    # 1. Text normalization: lowercase, remove HTML/tags/hyperlinks/emojis/special chars\n",
        "    article = article.lower()\n",
        "    article = re.sub(r'<.*?>', '', article)  # Remove HTML tags\n",
        "    article = re.sub(r'http\\S+|www\\S+', '', article)  # Remove URLs\n",
        "    article = re.sub(r'[^\\w\\s]', '', article)  # Remove special chars/emojis\n",
        "    article = re.sub(r'\\s+', ' ', article).strip()  # Standardize whitespace\n",
        "\n",
        "    # 2. Sentence segmentation with spaCy\n",
        "    doc = nlp(article)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 0]\n",
        "\n",
        "    # 3. Tokenization with BERT WordPiece\n",
        "    tokenized_sentences = []\n",
        "    for sent in sentences:\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        # Stopword filtering: remove stopwords for similarity computations later\n",
        "        filtered_tokens = [tok for tok in tokens if tok not in stop_words]\n",
        "        tokenized_sentences.append(filtered_tokens)\n",
        "\n",
        "    # 4. Rare token handling: Replace tokens with frequency < 2 with <UNK> (corpus-level, but for simplicity, per article)\n",
        "    # Build freq dict\n",
        "    all_tokens = [tok for sent in tokenized_sentences for tok in sent]\n",
        "    freq = nltk.FreqDist(all_tokens)\n",
        "    for i, sent in enumerate(tokenized_sentences):\n",
        "        tokenized_sentences[i] = [tok if freq[tok] > 1 else '<UNK>' for tok in sent]\n",
        "\n",
        "    return sentences, tokenized_sentences\n"
      ],
      "metadata": {
        "id": "WknYSaP8fwA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, split='train'):\n",
        "        self.data = hf_dataset[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.data[idx]['article']\n",
        "        summary = self.data[idx]['highlights']  # Ground truth summary\n",
        "        sentences, tokenized_sentences = preprocess_article(article)\n",
        "        return {\n",
        "            'article': article,\n",
        "            'summary': summary,\n",
        "            'sentences': sentences,\n",
        "            'tokenized_sentences': tokenized_sentences\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SummarizationDataset(dataset, 'train')\n",
        "val_dataset = SummarizationDataset(dataset, 'validation')\n",
        "test_dataset = SummarizationDataset(dataset, 'test')\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DNiA2ZKof2Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GETSum(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, num_heads=8, num_gat_layers=2, dropout=0.1):\n",
        "        super(GETSum, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.gat_layers = nn.ModuleList([GATConv(hidden_dim, hidden_dim // num_heads, num_heads) for _ in range(num_gat_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Gating mechanism for fusion\n",
        "        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)  # For concatenation + sigmoid gating\n",
        "\n",
        "        # For abstractive: Simple transformer decoder (e.g., using nn.TransformerDecoder)\n",
        "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=12), num_layers=6)\n",
        "        self.fc_out = nn.Linear(hidden_dim, tokenizer.vocab_size)  # Output to vocab size for generation\n",
        "\n",
        "    def forward(self, batch, mode='abstractive'):\n",
        "        # Tokenize full article for BERT\n",
        "        inputs = tokenizer(batch['article'], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "        # 1. Transformer Encoder: Get contextual embeddings\n",
        "        with torch.no_grad():  # Freeze BERT initially\n",
        "            bert_outputs = self.bert(**inputs)\n",
        "        word_embeddings = bert_outputs.last_hidden_state  # (batch, seq_len, 768)\n",
        "\n",
        "        # Get sentence embeddings: Average word embeddings per sentence\n",
        "        sentence_embeddings = []\n",
        "        for i, sents in enumerate(batch['sentences']):\n",
        "            sent_embs = []  # Per article\n",
        "            for sent in sents:\n",
        "                sent_tokens = tokenizer(sent, return_tensors='pt').to(device)\n",
        "                sent_out = self.bert(**sent_tokens).last_hidden_state.mean(dim=1)  # Avg pooling\n",
        "                sent_embs.append(sent_out)\n",
        "            sentence_embeddings.append(torch.stack(sent_embs))\n",
        "\n",
        "        # Pad sentence_embeddings to same num_sentences (for batching, assume max_sents=50 for simplicity)\n",
        "        max_sents = max(len(s) for s in sentence_embeddings)\n",
        "        padded_sents = [torch.cat([s, torch.zeros(max_sents - len(s), 768).to(device)], dim=0) for s in sentence_embeddings]\n",
        "        sentence_embeddings = torch.stack(padded_sents).to(device)  # (batch, max_sents, 768)\n",
        "\n",
        "        # 2. Graph Construction: Nodes = sentences, Edges = cosine sim > 0.3\n",
        "        graphs = []\n",
        "        for i in range(batch_size):\n",
        "            sim_matrix = cosine_similarity(sentence_embeddings[i].cpu().numpy())\n",
        "            adj_matrix = (sim_matrix > 0.3).astype(float)  # Threshold 0.3\n",
        "            src, dst = np.nonzero(adj_matrix)\n",
        "            g = dgl.graph((src, dst), num_nodes=max_sents).to(device)\n",
        "            g.ndata['feat'] = sentence_embeddings[i]\n",
        "            graphs.append(g)\n",
        "        batched_graph = dgl.batch(graphs)\n",
        "\n",
        "        # 3. Graph Attention Network (GAT)\n",
        "        h = batched_graph.ndata['feat']\n",
        "        for layer in self.gat_layers:\n",
        "            h = layer(batched_graph, h).flatten(1)  # Multi-head concat\n",
        "            h = self.dropout(h)\n",
        "\n",
        "        # Unbatch and pad back\n",
        "        unbatched_h = dgl.unbatch(batched_graph)\n",
        "        gat_embeddings = [uh.ndata['feat'] for uh in unbatched_h]  # List of (num_sents, 768)\n",
        "\n",
        "        # 4. Representation Integration: Concat + Gating\n",
        "        # For simplicity, average word_embeddings to document level, but align to sentence level\n",
        "        # Assume sentence_embeddings from BERT is used for fusion\n",
        "        fused_embeddings = []\n",
        "        for i in range(batch_size):\n",
        "            concat = torch.cat([sentence_embeddings[i], gat_embeddings[i]], dim=1)  # (num_sents, 1536)\n",
        "            gate_weight = torch.sigmoid(self.gate(concat))  # (num_sents, 768)\n",
        "            fused = gate_weight * sentence_embeddings[i] + (1 - gate_weight) * gat_embeddings[i]\n",
        "            fused_embeddings.append(fused)\n",
        "        fused_embeddings = torch.stack(fused_embeddings)  # (batch, max_sents, 768)\n",
        "\n",
        "        # 5. Summary Generation\n",
        "        if mode == 'extractive':\n",
        "            # Rank sentences: Use fused scores (e.g., sum over dim=2)\n",
        "            scores = fused_embeddings.sum(dim=2)  # (batch, max_sents)\n",
        "            top_k = torch.topk(scores, k=3, dim=1).indices  # Top 3-5 sentences\n",
        "            return top_k  # For extractive: indices to select sentences\n",
        "\n",
        "        elif mode == 'abstractive':\n",
        "            # Feed fused to decoder (target: tokenized summary)\n",
        "            tgt_inputs = tokenizer(batch['summary'], padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "            decoder_out = self.decoder(tgt_inputs.input_ids, fused_embeddings.mean(dim=1).unsqueeze(1))  # Simplified\n",
        "            logits = self.fc_out(decoder_out)\n",
        "            return logits\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Mode must be 'extractive' or 'abstractive'\")\n",
        "\n",
        "# Instantiate model\n",
        "model = GETSum().to(device)"
      ],
      "metadata": {
        "id": "NgbklsD1f589"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "extractive_loss_fn = nn.MarginRankingLoss(margin=1.0)  # Pairwise ranking for extractive\n",
        "abstractive_loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "def train_epoch(loader, model, optimizer, mode='abstractive', epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode == 'extractive':\n",
        "            # For extractive: Need ground truth sentence labels (simulate: assume first few sentences are positive)\n",
        "            # In practice, use ROUGE-based oracle labels; here simplified\n",
        "            outputs = model(batch, mode='extractive')  # Top indices\n",
        "            # Pairwise loss: Compare positive/negative pairs (dummy example)\n",
        "            pos_scores = torch.ones(batch_size).to(device)  # Placeholder\n",
        "            neg_scores = torch.zeros(batch_size).to(device)\n",
        "            loss = extractive_loss_fn(pos_scores, neg_scores, torch.ones(batch_size).to(device))\n",
        "\n",
        "        elif mode == 'abstractive':\n",
        "            logits = model(batch, mode='abstractive')\n",
        "            tgt = tokenizer(batch['summary'], padding=True, truncation=True, return_tensors='pt')['input_ids'].to(device)\n",
        "            loss = abstractive_loss_fn(logits.view(-1, tokenizer.vocab_size), tgt.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Avg Loss = {total_loss / len(loader)}\")\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Fine-tune for 10 epochs (as per document for CNN/DM)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(train_loader, model, optimizer, mode='hybrid')  # Switch mode as needed\n",
        "    # Validate similarly on val_loader (implement eval function for ROUGE/BERTScore)\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), 'getsum_model.pth')"
      ],
      "metadata": {
        "id": "16dQ9dFVf9su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement evaluation metrics (ROUGE, BERTScore)\n",
        "# !pip install rouge-score bert-score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "\n",
        "def evaluate(model, loader, mode='abstractive'):\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if mode == 'extractive':\n",
        "                top_indices = model(batch, mode='extractive')\n",
        "                pred_summary = [' '.join([batch['sentences'][i][idx] for idx in top_indices[i]]) for i in range(batch_size)]\n",
        "            elif mode == 'abstractive':\n",
        "                logits = model(batch, mode='abstractive')\n",
        "                pred_ids = torch.argmax(logits, dim=-1)\n",
        "                pred_summary = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "            predictions.extend(pred_summary)\n",
        "            references.extend(batch['summary'])\n",
        "\n",
        "    # ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {k: np.mean([scorer.score(ref, pred)[k].fmeasure for ref, pred in zip(references, predictions)]) for k in ['rouge1', 'rouge2', 'rougeL']}\n",
        "\n",
        "    # BERTScore\n",
        "    P, R, F1 = score(predictions, references, lang='en', verbose=True)\n",
        "    bertscore = F1.mean().item()\n",
        "\n",
        "    print(f\"ROUGE: {rouge_scores}\\nBERTScore: {bertscore}\")\n",
        "\n",
        "# Example: Evaluate on test\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "id": "4C6vRUXngDuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXfefA9vgJSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}